{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[{"file_id":"1VlxFVYAlEhzr3KhATprf2RBYCQ6UK-a5","timestamp":1753782320747}],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"OprsgDBdEysP"},"outputs":[],"source":["!pip install transformers datasets torch pandas scikit-learn matplotlib seaborn\n"]},{"cell_type":"code","source":["# Step 2: Import necessary libraries\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from datasets import load_dataset, Dataset\n","from transformers import (\n","    AutoTokenizer,\n","    AutoModelForSequenceClassification,\n","    TrainingArguments,\n","    Trainer,\n","    pipeline\n",")\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n","import torch\n","from torch.utils.data import DataLoader\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","print(\"‚úÖ All libraries imported successfully!\")"],"metadata":{"id":"ElIi0OhrEzki"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Step 3: Load and explore the dataset\n","print(\"üìä Loading dataset...\")\n","dataset = load_dataset(\"sharjeelyunus/github-issues-dataset\")\n","\n","# Convert to pandas for easier exploration\n","df = dataset['train'].to_pandas()\n","print(f\"Dataset shape: {df.shape}\")\n","print(f\"Columns: {df.columns.tolist()}\")\n","print(\"\\nüìã First few rows:\")\n","print(df.head())\n","print(df.tail())"],"metadata":{"id":"Tn8UR94lHmjf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Step 4: Data exploration and preprocessing\n","print(\"\\nüîç Dataset Info:\")\n","print(df.info())\n","print(\"\\nüìà Target distribution:\")\n","if 'severity' in df.columns:\n","    print(df['severity'].value_counts())\n","elif 'label' in df.columns:\n","    print(df['label'].value_counts())\n","else:\n","    print(\"Available columns:\", df.columns.tolist())\n","    print(\"Please check which column contains severity labels\")\n","\n","# Check for missing values\n","print(f\"\\n‚ùå Missing values:\\n{df.isnull().sum()}\")"],"metadata":{"id":"aH1ueFM2Hztx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Step 5: Prepare data for BERT\n","# Combine title and body for better context (adjust column names as needed)\n","def prepare_text_data(df):\n","    \"\"\"Prepare text data by combining relevant fields\"\"\"\n","    text_columns = []\n","\n","    # Check available text columns\n","    if 'title' in df.columns:\n","        text_columns.append('title')\n","    if 'body' in df.columns:\n","        text_columns.append('body')\n","    if 'description' in df.columns:\n","        text_columns.append('description')\n","\n","    if len(text_columns) == 0:\n","        # If no standard columns, use the first text-like column\n","        for col in df.columns:\n","            if df[col].dtype == 'object':\n","                text_columns.append(col)\n","                break\n","\n","    # Combine text fields\n","    if len(text_columns) > 1:\n","        df['combined_text'] = df[text_columns].fillna('').apply(\n","            lambda x: ' '.join(x.astype(str)), axis=1\n","        )\n","    else:\n","        df['combined_text'] = df[text_columns[0]].fillna('')\n","\n","    return df\n","\n","df = prepare_text_data(df)"],"metadata":{"id":"WwFJG-T-H9WH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Step 6: Define severity mapping (adjust based on your dataset)\n","def map_severity_labels(df):\n","    \"\"\"Map severity labels to numerical values\"\"\"\n","    severity_col = None\n","\n","    # Find severity column\n","    for col in ['severity', 'label', 'priority', 'level']:\n","        if col in df.columns:\n","            severity_col = col\n","            break\n","\n","    if severity_col is None:\n","        print(\"‚ö†Ô∏è No severity column found. Creating dummy labels for demo.\")\n","        df['severity'] = np.random.choice(['low', 'medium', 'high'], size=len(df))\n","        severity_col = 'severity'\n","\n","    # Create label mapping\n","    unique_labels = df[severity_col].unique()\n","    label_to_id = {label: idx for idx, label in enumerate(unique_labels)}\n","    id_to_label = {idx: label for label, idx in label_to_id.items()}\n","\n","    df['labels'] = df[severity_col].map(label_to_id)\n","\n","    print(f\"üìù Label mapping: {label_to_id}\")\n","    return df, label_to_id, id_to_label\n","\n","df, label_to_id, id_to_label = map_severity_labels(df)"],"metadata":{"id":"wR_7B-syIEks"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Step 7: Split the data\n","print(\"‚úÇÔ∏è Splitting data...\")\n","train_df, test_df = train_test_split(\n","    df,\n","    test_size=0.2,\n","    random_state=42,\n","    stratify=df['labels']\n",")\n","\n","train_df, val_df = train_test_split(\n","    train_df,\n","    test_size=0.2,\n","    random_state=42,\n","    stratify=train_df['labels']\n",")\n","\n","print(f\"Training set: {len(train_df)} samples\")\n","print(f\"Validation set: {len(val_df)} samples\")\n","print(f\"Test set: {len(test_df)} samples\")"],"metadata":{"id":"D_rDhkuDIPII"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Step 8: Initialize BERT tokenizer and model\n","print(\"ü§ñ Loading BERT model...\")\n","model_name = \"bert-base-uncased\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = AutoModelForSequenceClassification.from_pretrained(\n","    model_name,\n","    num_labels=len(label_to_id)\n",")"],"metadata":{"id":"27cXWIwEJacf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Step 9: Memory-efficient tokenization\n","def tokenize_data_batch(texts, tokenizer, max_length=256, batch_size=1000):\n","    \"\"\"Memory-efficient tokenization with batching\"\"\"\n","    print(f\"üî§ Tokenizing {len(texts)} texts in batches of {batch_size}...\")\n","\n","    all_input_ids = []\n","    all_attention_masks = []\n","\n","    # Process in smaller batches\n","    for i in range(0, len(texts), batch_size):\n","        batch_texts = texts[i:i + batch_size].tolist()\n","        print(f\"  Processing batch {i//batch_size + 1}/{(len(texts)-1)//batch_size + 1}\")\n","\n","        # Tokenize batch\n","        batch_encoding = tokenizer(\n","            batch_texts,\n","            truncation=True,\n","            padding=True,\n","            max_length=max_length,\n","            return_tensors=\"pt\"\n","        )\n","\n","        # Store results\n","        all_input_ids.append(batch_encoding['input_ids'])\n","        all_attention_masks.append(batch_encoding['attention_mask'])\n","\n","        # Clear memory\n","        del batch_encoding\n","        torch.cuda.empty_cache() if torch.cuda.is_available() else None\n","\n","    # Combine all batches\n","    final_input_ids = torch.cat(all_input_ids, dim=0)\n","    final_attention_masks = torch.cat(all_attention_masks, dim=0)\n","\n","    return {\n","        'input_ids': final_input_ids,\n","        'attention_mask': final_attention_masks\n","    }\n","\n","# SOLUTION 1: Reduce dataset size for testing\n","print(\"üîç Checking dataset size...\")\n","if len(df) > 20000:\n","    print(f\"‚ö†Ô∏è Large dataset detected ({len(df)} samples). Using subset for training.\")\n","    df_subset = df.sample(n=20000, random_state=42)  # Use 20k samples\n","    train_df, test_df = train_test_split(df_subset, test_size=0.2, random_state=42, stratify=df_subset['labels'])\n","    train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42, stratify=train_df['labels'])\n","    print(f\"‚úÖ Using subset - Training: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}\")\n","\n","# SOLUTION 2: Clean up memory before tokenization\n","import gc\n","gc.collect()\n","torch.cuda.empty_cache() if torch.cuda.is_available() else None\n","\n","print(\"üî§ Starting memory-efficient tokenization...\")\n","# Reduced max_length to save memory (256 instead of 512)\n","train_encodings = tokenize_data_batch(train_df['combined_text'], tokenizer, max_length=256, batch_size=500)\n","print(\"‚úÖ Training data tokenized\")\n","\n","val_encodings = tokenize_data_batch(val_df['combined_text'], tokenizer, max_length=256, batch_size=500)\n","print(\"‚úÖ Validation data tokenized\")\n","\n","test_encodings = tokenize_data_batch(test_df['combined_text'], tokenizer, max_length=256, batch_size=500)\n","print(\"‚úÖ Test data tokenized\")\n","\n","# Clear memory after tokenization\n","gc.collect()\n","torch.cuda.empty_cache() if torch.cuda.is_available() else None"],"metadata":{"id":"JwN8MHFBJd_2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Step 10: Create Dataset class\n","class IssueDataset(torch.utils.data.Dataset):\n","    def __init__(self, encodings, labels):\n","        self.encodings = encodings\n","        self.labels = labels\n","\n","    def __getitem__(self, idx):\n","        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","        item['labels'] = torch.tensor(self.labels[idx])\n","        return item\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","# Create datasets\n","train_dataset = IssueDataset(train_encodings, train_df['labels'].tolist())\n","val_dataset = IssueDataset(val_encodings, val_df['labels'].tolist())\n","test_dataset = IssueDataset(test_encodings, test_df['labels'].tolist())"],"metadata":{"id":"g9YUbXNZNcPr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Step 11: Safe training arguments (multiple error fixes)\n","import os\n","\n","# Create output directory if it doesn't exist\n","os.makedirs('./results', exist_ok=True)\n","os.makedirs('./logs', exist_ok=True)\n","\n","# SAFE training arguments with error handling\n","training_args = TrainingArguments(\n","    output_dir='./results',\n","    num_train_epochs=2,\n","    per_device_train_batch_size=2,  # Further reduced to prevent memory issues\n","    per_device_eval_batch_size=4,\n","    warmup_steps=100,\n","    weight_decay=0.01,\n","    logging_dir='./logs',\n","    logging_steps=50,\n","    eval_strategy=\"epoch\",\n","    save_strategy=\"epoch\",\n","    load_best_model_at_end=True,\n","    metric_for_best_model=\"eval_loss\",\n","    greater_is_better=False,\n","    # Removed potentially problematic arguments\n","    gradient_accumulation_steps=4,  # Increased to compensate for smaller batch\n","    remove_unused_columns=False,  # Prevent column removal issues\n","    report_to=None,  # Disable wandb/tensorboard to avoid errors\n",")\n","print(\"‚úÖ Training arguments created successfully!\")\n","\n","\n","# Verify datasets exist before proceeding\n","print(f\"üìä Dataset sizes:\")\n","print(f\"  Training: {len(train_dataset)} samples\")\n","print(f\"  Validation: {len(val_dataset)} samples\")\n","print(f\"  Test: {len(test_dataset)} samples\")"],"metadata":{"id":"m7KfabN_iP9t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Step 12: Define metrics\n","def compute_metrics(eval_pred):\n","    predictions, labels = eval_pred\n","    predictions = np.argmax(predictions, axis=1)\n","    return {\n","        'accuracy': accuracy_score(labels, predictions)\n","    }"],"metadata":{"id":"rzoC5ciMid3Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Step 13: Initialize trainer\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=val_dataset,\n","    compute_metrics=compute_metrics,\n",")"],"metadata":{"id":"LgQ8UdJwijwq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Step 14: Train the model\n","print(\"üöÄ Starting training...\")\n","trainer.train()"],"metadata":{"id":"32HP8nF5imhM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Step 15: Evaluate on test set\n","print(\"üìä Evaluating on test set...\")\n","test_results = trainer.evaluate(test_dataset)\n","print(f\"Test Results: {test_results}\")"],"metadata":{"id":"zAOZ7oIviruI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Step 16: Make predictions\n","predictions = trainer.predict(test_dataset)\n","y_pred = np.argmax(predictions.predictions, axis=1)\n","y_true = test_df['labels'].tolist()"],"metadata":{"id":"9zNuCbfVi3KA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Step 17: Generate detailed results\n","print(\"üìà Classification Report:\")\n","print(classification_report(y_true, y_pred, target_names=list(label_to_id.keys())))"],"metadata":{"id":"UyFwyMEMi4lU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Confusion Matrix\n","plt.figure(figsize=(8, 6))\n","cm = confusion_matrix(y_true, y_pred)\n","sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n","            xticklabels=list(label_to_id.keys()),\n","            yticklabels=list(label_to_id.keys()))\n","plt.title('Confusion Matrix')\n","plt.ylabel('True Label')\n","plt.xlabel('Predicted Label')\n","plt.show()"],"metadata":{"id":"VEXZS2rxi5v1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Step 18: Save the model\n","print(\"üíæ Saving model...\")\n","model.save_pretrained('./issue-severity-classifier')\n","tokenizer.save_pretrained('./issue-severity-classifier')"],"metadata":{"id":"K6-YvY06i7CU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Step 19: Create inference pipeline\n","print(\"üéØ Creating inference pipeline...\")\n","classifier = pipeline(\n","    \"text-classification\",\n","    model=model,\n","    tokenizer=tokenizer,\n","    return_all_scores=True\n",")"],"metadata":{"id":"upc0PezWjKBF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Step 20: Test with sample predictions\n","def predict_severity(text):\n","    \"\"\"Predict severity for a given text\"\"\"\n","    results = classifier(text)\n","    predictions = {id_to_label[i]: score['score'] for i, score in enumerate(results[0])}\n","    predicted_label = max(predictions, key=predictions.get)\n","    confidence = max(predictions.values())\n","\n","    return predicted_label, confidence, predictions"],"metadata":{"id":"LLukmssLjN3y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Example predictions\n","sample_issues = [\n","    \"Critical bug: Application crashes on startup\",\n","    \"Enhancement: Add dark mode theme\",\n","    \"Minor typo in documentation\"\n","]\n","\n","print(\"üîÆ Sample Predictions:\")\n","for issue in sample_issues:\n","    pred_label, confidence, all_scores = predict_severity(issue)\n","    print(f\"\\nIssue: '{issue}'\")\n","    print(f\"Predicted Severity: {pred_label} (Confidence: {confidence:.3f})\")\n","    print(f\"All scores: {all_scores}\")"],"metadata":{"id":"I_3w806rjWY8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Step 21: Model performance summary\n","print(\"\\n\" + \"=\"*50)\n","print(\"üéâ MODEL TRAINING COMPLETE!\")\n","print(\"=\"*50)\n","print(f\"‚úÖ Final Test Accuracy: {accuracy_score(y_true, y_pred):.3f}\")\n","print(f\"üìÅ Model saved to: './issue-severity-classifier'\")\n","print(f\"üè∑Ô∏è Number of classes: {len(label_to_id)}\")\n","print(f\"üìä Training samples: {len(train_df)}\")\n","print(f\"üîç Test samples: {len(test_df)}\")"],"metadata":{"id":"nfJcKYF7jQZa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Step 22: Test with new, unseen data\n","new_issues = [\n","    \"\tOutput file size with -s or -g\",\n","    \"The login button is not working on the mobile app.\",\n","    \"integrate template special sequences in help output\",\n","    \"Download range\",\n","    \"The API is returning a 500 error for all requests.\",\n","    \"The user interface is slow and unresponsive at times.\",\n","    \"The color of the main header is not consistent with the brand guidelines.\"\n","]\n","\n","print(\"üîÆ Predicting severity for new issues:\")\n","for issue in new_issues:\n","    pred_label, confidence, all_scores = predict_severity(issue)\n","    print(f\"\\nIssue: '{issue}'\")\n","    print(f\"Predicted Severity: {pred_label} (Confidence: {confidence:.3f})\")\n","    print(f\"All scores: {all_scores}\")"],"metadata":{"id":"dencQq14jZZl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"pOMHGAn2Iplc"},"execution_count":null,"outputs":[]}]}